diff --git a/detect.py b/detect.py
index f2d9f36..c67b4f2 100644
--- a/detect.py
+++ b/detect.py
@@ -7,6 +7,11 @@ import torch
 import torch.backends.cudnn as cudnn
 from numpy import random
 
+import sys
+import logging
+
+from torch2trt import torch2trt, tensorrt_converter, get_arg, trt, add_missing_trt_tensors, torch_dtype_to_trt
+
 from models.experimental import attempt_load
 from utils.datasets import LoadStreams, LoadImages
 from utils.general import check_img_size, non_max_suppression, apply_classifier, scale_coords, xyxy2xywh, \
@@ -14,6 +19,18 @@ from utils.general import check_img_size, non_max_suppression, apply_classifier,
 from utils.plots import plot_one_box
 from utils.torch_utils import select_device, load_classifier, time_synchronized
 
+# REGISTER NEW CONVERTERS
+@tensorrt_converter('torch.nn.functional.silu')
+def convert_silu(ctx):
+    input = get_arg(ctx, 'input', pos=0, default=None)
+    output = ctx.method_return
+    input_trt = add_missing_trt_tensors(ctx.network, [input])[0]
+    
+    layer = ctx.network.add_activation(input_trt, trt.ActivationType.SIGMOID)
+    layer = ctx.network.add_elementwise(input_trt, layer.get_output(0), trt.ElementWiseOperation.PROD)
+    
+    output._trt = layer.get_output(0)
+
 
 def detect(save_img=False):
     source, weights, view_img, save_txt, imgsz = opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size
@@ -32,6 +49,18 @@ def detect(save_img=False):
     # Load model
     model = attempt_load(weights, map_location=device)  # load FP32 model
     imgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size
+
+    x = torch.ones((1, 3, imgsz, imgsz), device=device)
+    try:
+        model_trt = torch2trt(model, [x], fp16_mode=True)
+    except:
+        logging.exception('could not create tensorRT model')
+        # import pdb
+        # t, v, tb = sys.exc_info()
+        # sys.last_type, sys.last_value, sys.last_traceback = t, v, tb
+        # pdb.pm()
+        exit()
+
     if half:
         model.half()  # to FP16
 
@@ -59,6 +88,7 @@ def detect(save_img=False):
     t0 = time.time()
     img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img
     _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once
+    _ = model_trt(img) if device.type != 'cpu' else None  # run once
     for path, img, im0s, vid_cap in dataset:
         img = torch.from_numpy(img).to(device)
         img = img.half() if half else img.float()  # uint8 to fp16/32
@@ -67,12 +97,17 @@ def detect(save_img=False):
             img = img.unsqueeze(0)
 
         # Inference
+        t_trt = time_synchronized()
+        pred = model_trt(img)[0]
+        print('trt pred (%.3fs)' % (time_synchronized() - t_trt))
+
         t1 = time_synchronized()
         pred = model(img, augment=opt.augment)[0]
+        t2 = time_synchronized()
+        print('torch pred. (%.3fs)' % (t2 - t1))
 
         # Apply NMS
         pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)
-        t2 = time_synchronized()
 
         # Apply Classifier
         if classify:
diff --git a/models/common.py b/models/common.py
index 0a1e7e3..d991fcf 100644
--- a/models/common.py
+++ b/models/common.py
@@ -41,7 +41,7 @@ class ImplicitA(nn.Module):
         nn.init.normal_(self.implicit, std=.02)
 
     def forward(self, x):
-        return self.implicit.expand_as(x) + x
+        return self.implicit.expand(x.size()) + x
 
 
 class ImplicitM(nn.Module):
@@ -52,7 +52,7 @@ class ImplicitM(nn.Module):
         nn.init.normal_(self.implicit, mean=1., std=.02)
 
     def forward(self, x):
-        return self.implicit.expand_as(x) * x
+        return self.implicit.expand(x.size()) * x
     
     
 class ReOrg(nn.Module):
@@ -236,7 +236,7 @@ class BottleneckCSPSE(nn.Module):
         self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)])
 
     def forward(self, x):
-        x = x * self.cvsig(self.cs(self.avg_pool(x))).expand_as(x)
+        x = x * self.cvsig(self.cs(self.avg_pool(x))).expand(x.size())
         y1 = self.cv3(self.m(self.cv1(x)))
         y2 = self.cv2(x)
         return self.cv4(self.act(self.bn(torch.cat((y1, y2), dim=1))))
@@ -259,7 +259,7 @@ class BottleneckCSPSEA(nn.Module):
         self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)])
 
     def forward(self, x):
-        x = x + x * self.cvsig(self.cs(self.avg_pool(x))).expand_as(x)
+        x = x + x * self.cvsig(self.cs(self.avg_pool(x))).expand(x.size())
         y1 = self.cv3(self.m(self.cv1(x)))
         y2 = self.cv2(x)
         return self.cv4(self.act(self.bn(torch.cat((y1, y2), dim=1))))
diff --git a/models/yolo.py b/models/yolo.py
index 256da5d..e4ac6b5 100644
--- a/models/yolo.py
+++ b/models/yolo.py
@@ -90,6 +90,7 @@ class IDetect(nn.Module):
         # x = x.copy()  # for profiling
         z = []  # inference output
         self.training |= self.export
+        print(f'doing training: {self.training}')
         for i in range(self.nl):
             x[i] = self.im[i](self.m[i](self.ia[i](x[i])))  # conv
             bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)
